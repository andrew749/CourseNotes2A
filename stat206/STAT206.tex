\documentclass[11pt]{article}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}

\title{STAT206}
\author{Andrew Codispoti}

\begin{document}
\maketitle
\section{Descriptive and Inferential Statistics Terminology}
\begin{enumerate}
  \item Statistics: Science of conducting studies to collect, organize,
    summarize, analyze and draw conclusions from data.
  \item
    variable:attribute can be diff values
  \item Categorical variables(Gender), Quantitative variables (age weight)
  \item Discrete var: values that can be counted
  \item continuous var: assume all values between any two specific values,
    measured
  \item random: vals determined by chance
  \item Data set:collection of data values
  \item descriptive statistics: describe situation in data ( collect,
    organize summarize, present data)
  \item inferential statistics: make inference from samples to population
  \item population:all indviduals under study
  \item sample group selected
  \item statistical hyp: claim
  \item hypothesis testing: decision making process for evaluating a
    claim about population from samples
\end{enumerate}
\par study samples of pop and use inferential stats to make inference
about pop
\subsection{Freq distribution and graphs}
\par raw data in original form organized in some way  (i.e.\ freq
distribution). can then be presented in graphs or charts
\par freq.\ dist organizes raw data into classes and frequencies.
\par range of data is large (lower class limit, upper class limit, class
boundaries, class width, class midpoint, open-ended distribution)
\subsection{General rules for grouped freq.\ dist.}
\par 5-20 classes, class width should be odd to ensure midpoints are
integers, mutually exclusive(non overlapping), continuos, exhaustive,
equal in width
\subsection{general procedure}
\begin{enumerate}
  \item determine classes
    \begin{enumerate}
      \item find highest(H) and lowest(L) values,
      \item range = H - L
      \item number(N) of classes desired
      \item width =  R/N (round to whole)
      \item lowest class limit, convenient value less than or equal to the
        smallest data value.
      \item add W to get lowr limit of next class
      \item class boundaries
    \end{enumerate}
  \item tally data
  \item find freq.\ of tallies
  \item cumulative freq
\end{enumerate}
\subsection{Histogram}
\par histogram uses vertical bars to rep.\ freq of classes
\par different shapes bell shaped, uniform, j-shaped
reverse j-shaped, right and left skewed, bimodal, u-shaped
\subsection{Stem and leaf plots}
\par uses part of data value as stem and part as leaf to form groups
\par more informative because shows values
\par first arrange data in order, separate into classes then plot
\par look for peaks and gaps, shape of distribution, variability by looking
at spread
\par can put back to back to compare with other data set
\subsection{Descriptive Measures}
\par describe data with numerical measures
\par characteristic or measure from population is a parameter, from sample
data is called a statistic
\par Measures of central tendency(mean median, mode)
\textbf{don't round until done}
\subsubsection{Average}
\par sum divided by total
\begin{equation*}
  \mu = \bar{X} = \frac{\sum{X}}{N}
\end{equation*}
\par rounding rule: final answer should be rounded to one or more decimal
place than original
\par finding mean from grouped frequency distribution
\begin{equation}
  \bar{X} = \frac{\sum{f \cdot X_m}}{n}
\end{equation}
\par f is frequency and $X_m$ are midpoints of class
\subsubsection{Median}
\par ordered so called data array
\par half smaller half larger, position $\frac{n+1}{2}$
\par when even number of values, b/w two given data values
\subsubsection{Mode}
\par value that appears the most frequently
\par data set can have multiple or no modes
\subsubsection{properties of central tendency}
\begin{enumerate}
  \item mean needs all values of data, varies less than media and mode, unique,
    cannot be computed for open-ended freq.\ distribution, highly affected by outliers
  \item median find middle, whether data falls into upper or lower half of the
    distribution, can be calc for open data dist.\ less affected by outlier
  \item most typical case req., easier to compute, can be computed for categorical or nominal data, not unique
\end{enumerate}

\section{Variation}
\subsection{Measure of Variation}
\par range is simple measure of variability
\par \textbf{Variance} is measure of variability that uses all the data points,
avg.\ deviation of values from mean.
\par \textbf{population variance} $\sigma^2$ is avg.\ of squared deviations of values from population mean $\mu$
\begin{equation}
  \sigma^2 = \frac{\sum{(X-\mu)^2}}{N}
\end{equation}
\par where N represents total no.\ of values in pop
\par \textbf{sample variance} denoted by $s^2$ sum of squared deviations of the values from sample mean $\bar{X}$ divided by $(n-1)$
\begin{equation}
  s^2 = \frac {\sum{(X-\bar{X})^2}}{n-1}
\end{equation}
\par where n is no.\ values in sample
\par divided by n-1  b/c unbiased estimate of $\sigma^2$. standard deviation is square root of variance
\textbf{poulation standard deviation}
\begin{equation}
  \sigma  = \sqrt{\sigma^2} = \sqrt {\frac{\sum{(X-\bar{\mu})^2}}{n-1}}
\end{equation}
\par sample standard deviation
\begin{equation}
  \begin{split}
    s  = \sqrt{s^2} = \sqrt {\frac{\sum{(X-\bar{X})^2}}{n-1}} \\
    s^2 = \frac{\sum{X^2 - \frac{(\sum X)^2}{n}}}{n-1} \\
    s^2 = \frac{\sum{f \cdot X_m^2 - \frac{(\sum f \dot X_m)^2}{n}}}{n-1} \\
  \end{split}
\end{equation}
\subsection{Note}
\begin{enumerate}
  \item value of $s^2$ always +
  \item sum of (X - $\bar{X}$) is always 0
  \item larger value of $s^2$ or s, larger variability of data ( same with $\sigma$ )
\end{enumerate}
\section{Empirical(Normal) Rule}
\par distro.\ bell shaped
\par some \% of vals fall within 1 standard deviation of mean (X-nS, X+nS)
\section{Chebyshev's Theorem}
\par in an dist regardl of shape, proportion of value that fall within k standard deviations will be at least $1-\frac{1}{k^2}$where $k > 1$
\par subtract average from larger value, divide diff by standard deviation to get k, use cheb to get \%
\section{Measure of Position}
\par standard score / z-score subtract mean from observation and divide result by standard deviation $z = \frac {X - \bar{X}}{s}  or  \frac{X - \mu}{\sigma} $
\par number of standard deviations falls above or below mean
\subsection{Partition Values}
\subsubsection{percentile}
\par 100 equal groups
\subsubsection{decile}
\par 10 equal groups
\subsubsection{quartiles}
\par 4 equal groups
\subsection{Percentile Equations}
\par find the percentile of a piece of data
\begin{equation}
  Percentile = \frac{number of data value below X + 0.5}{n} * 100\%
\end{equation}
\par find the position of a percentil
\begin{equation}
  c = \frac{n \cdot p}{100}
\end{equation}
\par if c is not a whole number round to next whole number, and position is
the number.\ if c is a whole number average c and c+1. this is position of
X
\section{Box-Plots}
\par show distrivution of data
\par helpful for finding outliers in the data based on the min and max relative
the $Q_1$ and $Q_3$ measurements
\par modified box plots dont show outliers
\section{Product rule}
\par total number of possibilities are $k_1 \cdot k_2 \cdot \dots k_n$
\section{Permutation Rule}
\par permutation: arrangement of n objects in specific order
\par Rule 1: arrangement of n objects in specific order using r object of
time, denoted by nPr:
\begin{equation}
  nP_r = \frac{n!}{(n-r)!}
\end{equation}
\par arrangment of n objects in specific order taking all n at a time is n!
\section{Combination Rule}
\par Combination: selection of objects with no order
\par the selection of r objects out of n objects can be done in $nC_r$ ways
\begin{equation}
  \binom(n, r) = \frac{n!}{(n-r)!r!}
\end{equation}
\section{Combinatorics Ex}
\begin{equation}
  n = \frac{8!}{5!*3!} \\
  = \frac{5!*6*7*8}{1*2*3} \\
  = \frac{\frac{6!}{3!3!}}{56} + \frac{\frac{6!}{5!}}{56} \\
  = \frac{13}{28} \\
\end{equation}
\section{Bayes Theorem}
\begin{align*}
  P(A \cap B) = P(A)\cdot P(B|A)\\
  P(A|B)=\frac{P(A \cap B)}{P(B)}\\
  P(A|B)= \frac{P(Ai*P(D|Ai))}{P(Ai)P(D|Ai)+\dots+ P(Ak)P(D|Ak)}
\end{align*}
\section{Probability Distributions}
\begin{enumerate}
  \item variable: characteristic  or attribute that can assume different
    values
  \item Random Variable: variable whose value is determined by chance
  \item have requirment that $\sum P(X) = 1$, $0 \leq P(X) \leq 1$
  \item Expectation: $E(X) = \mu = \sum X \cdot P(X)$
\end{enumerate}
\section{The Binomial Distribution}
\par binomial experiment: probability experiment that satisfies:
\begin{enumerate}
  \item each trial can have only two outcomes or can be reduced to success or
    failure
  \item fixed \# of trials
  \item outcomes independant of each other
  \item probability of success must remain same for each trial
\end{enumerate}
\begin{align*}
  Mean: \mu = \sum (X \cdot P(X))\\
  Variance: \sigma^2 = \sum[X^2\cdot P(X)] -\mu^2\\
  Standard Deviation: \sigma = \sqrt{\sigma^2}\\
  Expectation: E(X) = \mu = \sum (X \cdot P(X))\\
\end{align*}
\section{Poisson distribution}
\par discrete probability distribution that is useful in large numbe of trials
with small success rate
\begin{equation}
  P(X;\lambda) = \frac{e^{-\lambda} \lambda^x}{X!} where X = 0,1,2
\end{equation}
\par where $\lambda = n \cdot p$ is the parameter of the poisson distribution
\begin{enumerate}
  \item Mean: $\mu = \lambda$
  \item Variance: $\sigma^2 = \lambda$
  \item Standard Deviation: $\sigma = \sqrt{\lambda}$
\end{enumerate}
\section{Poisson Process}
\par events occur randomly in time and space
\begin{enumerate}
  \item Independance: number of occurences in disjoint intervals are independant
  \item individuality: events occur singly P(two or more events occur simultaneously) = 0
  \item Homogeneity: events occur according to a uniform rate of intensity
\end{enumerate}
\par if events occur with average rate of $\lambda$ per unit of time, and X is
num of events which occur in t units of time, the X~Poisson($\lambda\cdot t$)
\begin{align*}
  f(x)  = \frac{e^{-\lambda t}(\lambda t)^x}{x!}\\
  E(x) = \lambda t\\
  Var(x) = \lambda t
\end{align*}
\section{Geometric Distribution}
\par Bernoulli trials completed until successful.
\par let X be no of independant Bernoulli trials until the first success including first success then X follow geo dist. X~Geom(p)
\begin{align*}
  f(x) = p(1-p)^{x-1}, x = 1,2,\dots\\
  E(X) = \frac{1}{p}\\
  Var(X) = \frac{1-p}{p^2}
\end{align*}
\subsection{Memoryless Property of Geometric Random Variable}
\par let X be a Geometric(p) R.V. and t1, t2 $\in$ R. then $P(X=t1+t2\ |
\ x=t1)=P(X>t2)$
\par buy a car and travel x kilometers.\ reliability does not depend on previous
examples
\begin{equation}
  LS = \frac{P( X \geq t_1 +t_2 and x \ge t_1)}{P(X \ge t1)}
\end{equation}
\section{Chebyshev's Theorem}
\par Let X be a R.V with E(X)  and  $Var(x) = \sigma^2 $ Then for any $\epsilon
> 0 $
\begin{equation}
  P(|X-E(X)| \ge \epsilon ) \le \frac{Var(x)}{\epsilon^2} \forall \epsilon >
  0
\end{equation}
\begin{equation}
  P(|X-E(X)< \epsilon) = 1-P(|X-E(X) \ge \epsilon) \ge
  1-\frac{VAR(X)}{\epsilon^2}
\end{equation}
\par Let $\epsilon = k\sigma$,where $k \ge 0$, $\sigma > 0-st deviation$
\begin{equation}
  P(|X-E(X)|<k\sigma)\ge 1 - \frac{\sigma^2}{k^2\sigma^2}
\end{equation}
\begin{equation}
  P(|X-E(X)|<k\sigma)\ge 1 - \frac{1} {k^2}
\end{equation}
\subsection{Example}
\par The \# of students who miss a friday class is a Random varible X with mean
15 and st.\ deviation 2. Find/estimate the probability.
\begin{equation}
  P(9 < x < 21) = P(-6 < x-15 < 6) = P(|X-15|\le 6) = 1 - \frac{1}{3^2}
\end{equation}
\subsection{Example 2}

\begin{align*}
  X~Bin(5, 1/2) ; Find P(|X-E(X)| < 2 \sigma)\\
  E[X] = n.p = 5 \cdot 1/2 = 2.5\\
  Var[X] = np(1-p) = 5 \cdot 1/2 1/2 = 5/4  = 1.25\\
  P(|X-2.5| < 2\cdot \sqrt{1.25}) \ge 1-1/4 = 0.75 \\
\end{align*}
\par actual probability
\begin{align*}
  P(|x-2.5|< 2\sqrt{1.25}>) = P(-2.236< x-2.5< 2.236)\\
  P(0.264 < X <4.37) = P(1\le x\le 4)= F(4)-F(0) = 0.9688 - 0.0313 = 0.9375
\end{align*}
\section{Continuous Random variables and distributions}
\par function from sample space to real numbers
\begin{equation}
  def: X:S \rightarrow  R  [a, b] \in R
\end{equation}
\par where R(X) is continuous and individual points in R must have 0
probability
\begin{enumerate}
  \item $P(X=x) = 0 for any x \in \Re$
  \item $P(a\leq X \leq b) = P(a < X < b)$
\end{enumerate}
\subsection P.d.f probabiility density function
\par for random variable X, denoted f(x), assigns probability to $x\in \Re(X)$
\begin{align*}
  P(a<X<b) &= \int_{a}^{b}{f(x)} dx, (a,b) \subseteq \Re(x) \\
  f(x) &= 0 , \forall x \in \Re(x) \\
  \int_{\infty}^{\infty} f(x) dx &= 1\\
\end{align*}
\subsection{Cumulative distribution function(cdf)}
\par cdf of continuous random variale X, denoted F(x) gives prob that X takes
on value less than or equal to x.
\begin{equation}
  F(x) = P(X\leq x) = P(X<x)
\end{equation}
\subsubsection{Properties}
\begin{enumerate}
  \item $F(-\infty) = 0$
  \item $F(\infty) = 1$
  \item F(x) non decreasing
\end{enumerate}
\subsection{Relationship b/w pdf and cdf}
\begin{align*}
  \int_{a}^{b}f(x) dx = P(a < X < b)\\
  = P(X < b) - P(X < a)\\
  = F(b) - F(a)
\end{align*}
\par fundamental theorem of calc $\frac{dF(x)}{dx} = f(x)$
\section{Continuous Uniform distribution}
\par the probability of any subinterval of the range is proportional to the
length of the interval(two sub-interv.\ with same length must have same
probability)
\begin{align*}
  f(X) = \frac{1}{b-a}, a\le x\le b\\
  F(x) =
  \begin{array}{ll}
    0 & x < a
    \\
    \frac{x-a}{b-a} & a\le x \le b\\
    1 & b < x
  \end{array}
\end{align*}
\subsection{Mean/expected value}
\begin{align*}
  E(X)= \int_{x}xf(x)dx = \mu\\
  E[g(x)] = \int_{x}g(x)f(x)dx\\
  E[aX+bY] = aE[x] +bE[y]\\
\end{align*}
\subsection{Variance}
\begin{align*}
  Var(X) = E[(X-E(X))^2]\\
  Var [aX1+bX2] = a^2Var(X1)  + b^2Var(X2)
\end{align*}
\subsection{Exponential Distribution}
\par Events occur according to a Poisson process, measure the inter arrival
time s b/w events. If X is the amount of  time until next event in a posisson
process, $X~Exp(\theta)$ where $\theta = \frac{1}{\lambda}$
\begin{align*}
  f(x) = (1/\theta) e^{\-\frac{x}{\theta}}
  F(x) = 1 - e^{-\frac{x}{\theta}}
  E(x) = \theta
  Var(X) = \theta^2
\end{align*}
\section{Normal Distribution}
\begin{enumerate}
  \item range - to + infinity
  \item denote x $X~N(\mu, \sigma^2)$
\end{enumerate}

\begin{equation}
  pdf = f(x) = \frac{1}{\sqrt{2*\pi\sigma^2}}e^{-1/2(\frac{x-\mu}{\sigma})^2}
\end{equation}
\par mean and variance are parameters
\begin{enumerate}
  \item linear combination of independantly normally distributed parameters
    is normallydistributed.
  \item $P(Z>z) = P(Z<-z) $because of symmetry
  \item probabily density function for normally distributed randome variable
    is not integrable for anyfinite limits a,b.

\end{enumerate}
\begin{align*}
  E(X) = \mu\\
  Var(X) = \sigma^2
\end{align*}

\subsection{Central Limit Theorem}
\par use normal distrubtion to approximate probabilities for non-normal
distributions
\par independant random variables:have no influence on one another's values
\begin{equation}
  f(x,y) = P(X=x \cap Y=y) = P(X = x)P(Y=y) = f_x(x) f_y(y)
\end{equation}
\subsubsection{Sum}
\par let $X_a, X_2, \dots, X_n$ be independant random variables with same
distribution $E(X_i) = \mu$ and $Var(X_i) = \sigma^2$
\begin{equation}
  as n\rightarrow \infty,
  f \Sigma_iX_i approaches cdf for N(n\mu, n\sigma^2)
\end{equation}
\begin{equation}
  cdf of \frac{\Sigma_iX_i-n\mu}{\sigma\sqrt{n}} \rightarrow(0,1)
\end{equation}
\subsubsection{Average}
\par let $X_1, \dots, X_n$ be independant random variables, with same
distribution $E(X_i) = \mu$ and $Var(X_i) = \sigma^2$
\par as $n\rightarrow\infty$ cdf of random variable $X\bar = \Sigma_iX_i/n$
approaches cdf $N(\mu, \frac{\sigma^2}{n})$
\par cdf of randome var $\frac{X\bar - \mu}{\frac{\sigma}{\sqrt{n}}}$
approaches cdf for N(0,1)
\begin{enumerate}
  \item use it when n is large but finite.
  \item usually need n $\ge $30
  \item works for any variables unless $\mu or \sigma^2$ do not exist
\end{enumerate}
\subsubsection{Approximation to Binomial Distribution}
\begin{equation}
  \frac{X - n*p}{\sqrt{n*p*(1-p)}}
\end{equation}
\subsubsection{Continuity Correction}
\par improve approximation to sum or average of discrete random variables
using normal random variable.
\par go +- 0.5 for bar of width one as the integer value will be centered
\section{CI for $\mu$ when $\sigma $ known or n $\ge $30}
\textbf{point estimate} of a parameter is a specific numeric value.\\
\textbf{interval estimate} : interval or range of values used to estimate
the parameter
\begin{equation}
  \bar X - z_{\alpha/2}(\frac{\sigma}{\sqrt{n}}) < \mu < \bar X +z_{\alpha/2}(\frac{\sigma}{\sqrt{n}})
\end{equation}
point estimate +- maximum error
$z_{\alpha/2}(\frac{\sigma}{\sqrt{n}})$  maximum error
$z_{\alpha/2}$ critical value
$\alpha$ level of significance
\subsection{Confidence level}
\par probability that interval estimate will contain the parameter
\subsection{confidence interval}
\par specific interval estimate of a parameter determined by using data
obtained from sample and by usign specific confidence interval of
estimate.
\subsection{Sample size}
\par formula for minimum sample size needed for an interval estimate of
population mean. E is maximum error
\begin{equation}
  n=(\frac{z_{\alpha/2} \cdot \sigma}{E})^2
\end{equation}
\section{CI for $\mu$ when $\sigma $ known or n < 30}
\begin{equation}
  \bar X - t_{\alpha/2}(\frac{s}{\sqrt{n}}) < \mu < \bar X
  +t_{\alpha/2}(\frac{s}{\sqrt{n}})
\end{equation}
$t_{\alpha/2}(\frac{\sigma}{\sqrt{n}})$  maximum error
\par where s is sample standard deviation and $t_{\alpha/2}$ are found in
t-table for n-1 degree of freedom
\section{CI and sample size for proportions}
\begin{enumerate}
  \item p = population proportion
  \item $\hat{p}$ = sample proportion or $\hat{p}= \frac{X}{n}$ x is \# of sample
    units wiht characteristic n sample size
\end{enumerate}
\subsection{sample size for proportions}
formula for minumum sample size needed for interval estimate of population
is
\begin{equation}
  n = \hat{p} \hat{q} (\frac{z_{\alpha/2}}{E}) , E is maximum error forproportion
\end{equation}
\section{Maximum likelihood estimator}
\par find derivative of the multipliction of the equatino for all
xi's; when multiple unknown parameters, take partial derivatives and
solve for unknown.
\subsection{Unbiased Estimator}
\begin{equation}
  E(\tilde{\theta}) = \theta
\end{equation}
\section{Confidence Intervals}
\begin{equation}
  \bar{X} - \bar{Y} \pm
  Z_{\alpha/2}\sqrt{\frac{\sigma_1^2}{n_1}+\frac{\sigma_2^2}{n_2}}, \sigma = S
  for small
\end{equation}
\section{Independant small samples}
\subsection{Pooled Estimator}
\begin{equation}
  S_p^2 = \frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1+n_2-2}
\end{equation}
\begin{equation}
  dof = n1+n2-2
\end{equation}
\section{Unequal variace dof}
\begin{equation}
  \frac{(\frac{S_1^2}{n_1}+\frac{S_2^2}{n_2})^2}{\frac{\frac{S_1^2}{n_1}}{n_1-1}+\frac{\frac{S_2^2}{n_2})^2}{n_2-1}}
\end{equation}
\section{Difference of means--Not Independant}
\begin{align*}
  d_i= x_i - y_i \\
  \frac{\bar{D} * \mu_D}{\frac{S_D}{\sqrt{n}}}~t_{n-1} \\
  S_D^2=\frac{\sum_{i=1}^n (D_i-\bar{D})^2}{n-1}
\end{align*}
\section{Hypthesis Testing}
\begin{enumerate}
  \item Specify single default hypothesis and check whether the data is
    unlikely under the hypothesis. Also called null hypothesis because it means
    a new treatment has no effect.H0
  \item Alternative hypothesis. H0 is not true.
  \item Test Statistic (discrepancy measure): some funciton fo data that is
    constructed to measure degree of agreement between the data and null
    hypothesis.
\end{enumerate}
\subsection{Types of Error}
\par TYPE 1 ERROR: reject H0 when it is true.
\par TYPE 2 ERROR: do not reject H0 when it is false.
\begin{align*}
  P(Type 1 Error) = \alpha (significance level)\\
  P(Type 2 Error) = 1 - \beta (P is the power of the test)
\end{align*}
\subsection{Steps}
\begin{enumerate}
  \item Step 1: Determine null and alternative hypothesis.
  \item Step 2: Choose a test statistic. $T= T(X_1\dots X_n;\theta)$.
  \item Step 3: Find observed value of T. $T_{obs}= T(x_1,\dots x_n;\theta)$.
  \item Step 4: Critical value or p-value. If critical value, find critical
    value for level of significance, and take into account if its two sided or
    not. For pvalue determine error amount for the observed value.
  \item Step 5: Compare crtical or pvalue with hypothesis and reject if
    necessary.
\end{enumerate}

\subsection{Hypothesis test for mean}
\begin{equation}
  Z = \frac{\bar{X}- \mu_0}{\frac{\sigma}{n}}
\end{equation}
\subsection{P-Value approach}
\par once find test statistic, find probability greater than.
\subsection{Population proportion}
\begin{equation}
  Z_{obs} = \frac{\hat{p} - p}{\sqrt{\frac{p*(1-p)}{n}}}
\end{equation}
\begin{equation}
  \hat{p} = \frac{X}{n}
\end{equation}
\section{Chi Squared}
\begin{equation}
  S^2=\frac{\sum_{i=1}^n (X_i-\bar{X})^2}{n-1}
\end{equation}
\begin{equation}
  U = \frac{(n-1)S^2}{\sigma^2}, n-1 dof
\end{equation}
\begin{equation}
  (\frac{(n-1)s^2}{\chi_{\alpha/2}^2},\frac{(n-1)s^2}{\chi_{1-\alpha/2}^2}
\end{equation}
\section{Tests about Independance}
\subsection{Pearson's Test}
\begin{equation}
  D  = \sum_{j=1}^m\frac{(O_j - E_j)^2}{E_j}
\end{equation}
\par where O is observed and E is expected. For a large n, D has an approximate
chi squared distribution.
\par with r x c table, (a-1)(b-1) degrees of freedom, a num rows, b num of columns
\section{Goodness of Fit Test}
\par Try to fit distribution to given data, use pearson's test statistic.
\section{Regression}
\begin{enumerate}
  \item Statistically Dependant
  \item independant
\end{enumerate}
\subsection{Definitions}
\begin{enumerate}
  \item $\rho$ population's correlation coefficient
  \item e- sample correlation coefficient
\end{enumerate}
\begin{align*}
  \rho &= \frac{cov(x,y)}{\sigma_x\sigma_y}\\
  cov(x,y) &= E[X-E(X)][Y-E(Y)]\\
  cov(x,x) &= E[X-E(X)]^2\\
  -1\le p \le 1
\end{align*}
\begin{enumerate}
  \item $p => +1$ strong positive correlation
  \item $p => -1$ strong negative correlation
  \item $p => 0$ weak correlation
  \item $p == 0$ no correlation
\end{enumerate}
\begin{equation}
  t_{cal} = r \sqrt{\frac{n-2}{1-r^2}}, n-2
\end{equation}

\begin{align*}
  S_{xx}&= \sum_{i=1}^n(x_i-\bar{x})^2  =
  \sum_{i=1}^nx_i^2-\frac{(\sum_{i=1}^nx_i)^2}{n}\\
  S_{yy} &= \sum_{i=1}^n(y_i-\bar{y})^2  =
  \sum_{i=1}^ny_i^2-\frac{(\sum_{i=1}^ny_i)^2}{n}\\
  S_{xy} &= \sum_{i=1}^n (x_i=\bar{x})(y_i-\bar{y}) =
  \sum_{i=1}^nx_iy_i-\frac{(\sum_{i=1}^nx_i)(\sum_{i=1}^ny_i)}{n}\\
  b &= \frac{S_{xy}}{S_{xx}}\\
  \bar{y} &= b\bar{x}+a\\
  s_e^2&=\frac{S_{yy}-(S_{xy})^2/S_{xx}}{n-2}\\
  t &= \frac{b-\beta}{\frac{s_e}{\sqrt{S_{xx}}}}\\
  & b\pm t_{\alpha/2}*s_e/\sqrt{S_{xx}}
\end{align*}

\section{Correlation}
\begin{align*}
  r &=
  \frac{1}{n-1}\sum_{i=1}^n(\frac{x_i-\bar{x}}{s_x})(\frac{y_i-\bar{y}}{s_y})\\
  r &= \frac{S_{xy}}{\sqrt{S_{xx}S_{yy}}}
\end{align*}

\end{document}
